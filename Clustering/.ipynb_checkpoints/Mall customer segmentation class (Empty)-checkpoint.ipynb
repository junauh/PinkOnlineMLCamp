{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "(updated v4)\n",
    "# Day 3 Unsupervised Learning: K Means Clustering\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=https://i.vas3k.ru/7w1.jpg>\n",
    "\n",
    "## What have we already learned?\n",
    "- Machine learning is the science of getting computers to act without being explicitly programmed\n",
    "- Supervised learning is the process of learning with training labels\n",
    "- Classification is the process of predicting a class given data points - K-Nearest Neighbor(KNN)\n",
    "- Regression is the process of predicting a value given data points - linear regression\n",
    "\n",
    "But what happens when we don't have labels for our data? Unsupervised Learning!\n",
    "\n",
    "## What is Unsupervised Learning\n",
    "- Finding a structure or hidden patterns in data sets without a label\n",
    "- No target outcome to predict/estimate\n",
    "- Algorithms include K-means, Hierarchical clustering, Non-negative matrix factorization (NMF), biclustering, Gaussian mixture models (GMM) and many more\n",
    "\n",
    "## Clustering\n",
    "\n",
    "### What is Clustering?\n",
    "The most popular and well-known type of unsupervised learning. Clustering algorithms try to find natural groupings in data - similar data points are considered in the same group. We call these groups clusters.\n",
    "\n",
    "### Business Uses\n",
    "- Behavioral segmentation\n",
    "    e.g. customer segmentation for a marketing campaign based on purchase history, digital (application, website or platform), or demographics and interest  \n",
    "- Anomaly detection\n",
    "    e.g. fraud detection of bank transactions based on outlier detection\n",
    "- Recommendation\n",
    "    e.g. content based recommender system to build a recommendation model based on features\n",
    "\n",
    "## What is K-means Clustering?\n",
    "\n",
    "![alt text](https://i.stack.imgur.com/cIDB3.png \"Logo Title Text 1\")\n",
    "\n",
    "K-Means clustering is a simple and widely-used clustering algorithm. Given the value k, it tries to build k  clusters from samples in the dataset. \n",
    "\n",
    "### How does it work?\n",
    "\n",
    "![alt text](http://konukoii.com/blog/wp-content/uploads/2017/01/RunyanKmeans.gif \"Logo Title Text 1\")\n",
    "1. Given set of n numbers of points P ($P = (P_1, P_2....,P_n)$)\n",
    "2. Define a random value K, number of clusters\n",
    "3. Place centroids $ C_1, C_2, ..., C_K $ at random locations\n",
    "4. Repeat until convergence\n",
    ">For each point $ P_i(x_i,y_i) $\n",
    ">1. find the neareset centroid $C_j(x_j,y_j)$:\n",
    "><img src=https://muthu.co/wp-content/uploads/2019/10/2d_euclidean_distance_illustration.png width=\"400\">\n",
    ">e.g. Euclidean distance min $d(P_i,C_j) = \\sqrt{(x_i-x_j)^2 + (y_i-y_j)^2}$\n",
    ">2. Assign the point $ P_i$ to cluster which minimizes within-cluster variance \n",
    "\n",
    "    >For each cluster j = 1...K\n",
    "    >1. Find the mean of the distance values for each cluster K - sum of all the distances between $C_j$ and the data points assigned to the cluster $P_i$ and devide by the number of data points within the cluster\n",
    "    > $$  \\frac{1}{n_i}\\sum{d(P_i,C_j)}$$\n",
    "    >2. Assign the mean value to the centroid\n",
    "    > $$ min(\\frac{1}{n_i}\\sum{d(P_i,C_j)}) $$\n",
    "    \n",
    "5. Stop once none of the cluster assignments change or reach the iteration budget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "x1 = 2\n",
    "y1 = 2\n",
    "x2 = -2\n",
    "y2 = -3\n",
    "\n",
    "# visually show the distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://muthu.co/wp-content/uploads/2019/10/2d_euclidean_distance_illustration.png width=\"400\">\n",
    "e.g. Euclidean distance min $d(P_i,C_j) = \\sqrt{(x_i-x_j)^2 + (y_i-y_j)^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#calculate euclidean distance using math.sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    x.  y.\n",
    "P = (2, -2) #x1, y1\n",
    "C = (3, 3)  #x2, y2\n",
    "\n",
    "#zip pairs items together which were passed into the iterator together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function called dist which calculates the euclidean distance between two points\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "# create simulated clusters using scikit learn's make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a scatter plot of the random blob data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign color to each cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that we have viewed the cluster, we will try to visualize assigning random cluster center to each cluster\n",
    "#show the scatter plot of the dataset as black as the starting point\n",
    "#assign color to the cluster centers\n",
    "#df.sample() returns n random samples or data points from the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that computes distances between three cluster centers to a data point\n",
    "# and find which cluster is closest to the data point and assign the cluster to the data point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So how does it work again?\n",
    "\n",
    "![alt text](http://konukoii.com/blog/wp-content/uploads/2017/01/RunyanKmeans.gif \"Logo Title Text 1\")\n",
    "1. Given set of n numbers of points P ($P = (P_1, P_2....,P_n)$)\n",
    "2. Define a random value K, number of clusters\n",
    "3. Place centroids $ C_1, C_2, ..., C_K $ at random locations\n",
    "4. Repeat until convergence\n",
    ">For each point $ P_i(x_i,y_i) $\n",
    ">1. find the neareset centroid $C_j(x_j,y_j)$:\n",
    "><img src=https://muthu.co/wp-content/uploads/2019/10/2d_euclidean_distance_illustration.png width=\"400\">\n",
    ">e.g. Euclidean distance min $d(P_i,C_j) = \\sqrt{(x_i-x_j)^2 + (y_i-y_j)^2}$\n",
    ">2. Assign the point $ P_i$ to cluster which minimizes within-cluster variance \n",
    "\n",
    "    >For each cluster j = 1...K\n",
    "    >1. Find the mean of the distance values for each cluster K - sum of all the distances between $C_j$ and the data points assigned to the cluster $P_i$ and devide by the number of data points within the cluster\n",
    "    > $$  \\frac{1}{n_i}\\sum{d(P_i,C_j)}$$\n",
    "    >2. Assign the mean value to the centroid\n",
    "    > $$ min(\\frac{1}{n_i}\\sum{d(P_i,C_j)}) $$\n",
    "    \n",
    "5. Stop once none of the cluster assignments change or reach the iteration budget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to determine the best value for k? \n",
    "\n",
    "When the K value is not unknown, there is the Elbow method is the very popular method to determine K.\n",
    "\n",
    "#### Elbow Method\n",
    "\n",
    "\n",
    "<img src=https://www.aionlinecourse.com/uploads/tutorials/2019/07/23_2_k_means_clustering.png width=\"700\">\n",
    "\n",
    "Intuitively, we can understand that increasing the number of K will reduce WCSS. So does that mean we should select the highest K that minimizes WCSS say as many clusters as the observations? NO.\n",
    "\n",
    "<img src=https://uc-r.github.io/public/images/analytics/clustering/kmeans/unnamed-chunk-12-1.png width=\"600\">\n",
    "\n",
    "This is where the 'Elbow method' comes in handy. The plot looks like an elbow with x-axis value with K and y-axis value with WCSS which stands for Within Cluster Sum of Squares - the sum of squares distances of each data point in all clusters to their respective centroids. The goal of the plot is to look for an 'elbow' point's K value in which the WCSS value drops significantly in the rate of change and tends to go parallel with the x-axis. The elbow point can then tell us the 'optimal' K value from both the application perspective as well as the computational performance perspective.\n",
    "\n",
    "But what do you do when your Elbow Method graph looks very ambiguous like below?\n",
    "\n",
    "<img src=https://miro.medium.com/max/1818/1*aI_dkLIlXW9EvpYjYcA8iQ.png width=\"700\">\n",
    "\n",
    "#### Silhouette Method\n",
    "\n",
    "We can cross-reference with other methods such as the Silhouette method in which uses both the inter and intracluster distances to cluster data points, unlike the elbow method which only uses intracluster distances.\n",
    "\n",
    "<img src=https://storage.googleapis.com/platform-blog-prod/silhouette/silhouette_formula.svg width=\"700\">\n",
    "\n",
    ">For each point $P_i$, calculate *average* distance to other points in same clusters $a(P_i)$\n",
    ">For each point $P_i$, calculate *averange* distance to nearest other cluster $b(P_i)$\n",
    ">NOTE! if $a(P_i)$ > $b(P_i)$, $P_i$ needs to be clustered again\n",
    "\n",
    "Silhouette score $$ S(P_i) = \\frac{b(P_i) - a(P_i)}{(max(a(P_i),b(P_i)))}$$\n",
    "\n",
    ">NOTE! ideally we want $a(P_i) = 0, b(P_i) = Inf, S(P_i) = 1 $\n",
    "\n",
    "Global silhouette score is defined as:\n",
    "\n",
    "<img src=https://gdcoder.com/content/images/2020/02/image-15.png width=\"700\">\n",
    "\n",
    "The goal of the method is to find K which maximizes a high average silhouette score, for the case below K should be 2!\n",
    "\n",
    "<img src=https://uc-r.github.io/public/images/analytics/clustering/kmeans/unnamed-chunk-14-1.png width=\"700\">\n",
    "\n",
    "### When should I use it?\n",
    "\n",
    "- If your data is numeric. It doesn't work with categorical features or text type features. We're computing the distance between real numbers!\n",
    "- If you don't have labels for your data\n",
    "- K-means is the simplest to implement and to run. All you need to do is choose \"k\" and run it a number of times.\n",
    "- K-means and other clustering algorithms shine when you have multivariate data. They will \"work\" with 1-dimensional data, but they are not very smart anymore.\n",
    "- Useful when you have an idea of how many clusters actually exists in your space. \n",
    "\n",
    "## Other examples\n",
    "\n",
    "- Fraud Detection: https://github.com/georgymh/ml-fraud-detection\n",
    "- MNIST without labels: https://github.com/Datamine/MNIST-K-Means-Clustering/blob/master/Kmeans.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Objective - Perform K-Means Clustering for Customer Segmentation in python\n",
    "\n",
    "### Objective\n",
    "Identify a customer group demography and buying habit in which has the highest potential to increase sales so the marketer can plan for a campaign specific to the group\n",
    "\n",
    "### Steps to take\n",
    "1. Explore data and confirm the model assumptions are met on data <br>\n",
    "1) numeric data set <br>\n",
    "2) no label found in data <br>\n",
    "\n",
    "2. Prepare data for clustering <br>\n",
    "1) remove categorical data from the data set <br>\n",
    "2) use visualization to understand data's skewedness <br>\n",
    "3) standardize data (mean = 0, std = 1)\n",
    "$$ X_{changed} = \\frac{X - \\mu}{\\sigma} $$\n",
    "standardization is required as K-means use the distance as the principal metric to cluster data - in other words, we need to standardize data and to ensure clustering is not affected by scale. Without standardization, for example, more 'relevance' or 'weight' might be given to large scale features and less 'relevance' or 'weight' might be given to small scale features\n",
    "\n",
    "3. Determine the optimum number of clusters, k value, using Elbow method and sihlouette method <br>\n",
    "1) iterate through a number of k values <br>\n",
    "2) run clustering for each k value on the same data <br>\n",
    "3) calculate the Within Cluster Sum of Squares (WCSS) for each k value <br>\n",
    "4) plot WCSS against k to identify elbow - diminishing incremental improvements in error reduction <br>\n",
    "5) plot sihlouette method graph to cross check the result\n",
    "\n",
    "4. Explore results of fitting k-means to the dataset while changing k values <br>\n",
    "1) calculate optimal k mathematically <br>\n",
    "2) build segmentation with multiple values around optimal k value <br>\n",
    "3) explore the results and choose the k value with the most business relevance (e.g. can you name the segment, are they ambiguous/overlapping?) <br>\n",
    "\n",
    "5. Share the outcome with the marketing manager and strategist to discuss marketing strategy outline for each customer segment <br>\n",
    "\n",
    "-  source: https://www.kaggle.com/vjchoudhary7/customer-segmentation-tutorial-in-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Explore data and confirm the K-means model assumptions are met\n",
    "\n",
    "# 1-1-a: import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# 1-1-b: read the dataset in csv format as a dataframe\n",
    "Mall_Customers = pd.read_csv('PinkOnlineSummerCamp/Clustering/data/Mall_Customers.csv')\n",
    "\n",
    "# 1-1-c: info() function to verify data type of each column\n",
    "\n",
    "# 1-1-d: head() function to see preview data set - ensure to check if there are any categorical column and no label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-1 define categorical data and keep only numerical data within the data set for\n",
    "# depending on the dataset one can define a categorical data as a column with\n",
    "# unique values less than 5 \n",
    "# ex. Mall_Customers = Mall_Customers.loc[:, Mall_Customers.nunique() > 5]\n",
    "# based on the above definition the Gender column is defined as a categorical feature.\n",
    "# However since it is a feature with binary values: Female and Male, we will simply convert\n",
    "\n",
    "# 2-1-a: Replace the text/categorical Gender values into numerical values Female = 2 and Male = 1\n",
    "\n",
    "# 2-1-b: Remove CustomerID as PII should not be included in any data set for GDPR compliance\n",
    "\n",
    "# 2-1-c. Review the result of the data frame cleaning \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-2 standardize data using Standardscalar (mean = 0, std = 1)\n",
    "\n",
    "# 2-2-a: check if data is already standardized (mean = 0, std = 1) by applying the describe() function and distplot\n",
    "\n",
    "# 2-2-b: import StandardScaler library as the dataset is not standardized (mean != 0, std != 1)\n",
    "\n",
    "# 2-2-c: initialize a scalar using StandardScaler()\n",
    "\n",
    "# 2-2-d: fit a scalar using fit()\n",
    "\n",
    "# 2-2-e: scale and center the data using transform()\n",
    "\n",
    "# 2-2-f: create a pandas DataFrame with the standardized data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-2-g: print summary statistics of the datset using describe() and distplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-1 Elbow method\n",
    "\n",
    "# 3-1-a: import KMeans library from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 3-1-b: initiate a list called wcss\n",
    "\n",
    "# 3-1-c-1: build a for loop looping through k values between 1,11\n",
    "    \n",
    "    # 3-1-c-2: initiate the instance of KMeans by setting the number of clusters as k, in order to replicate the\n",
    "    # result it is crucial to select any number but a number and define the random_state parameter\n",
    "    \n",
    "    # 3-1-c-3: use the fit() function on the standardized data set into the KMeans instance\n",
    "    \n",
    "    # 3-1-c-4: append inertia_ attribute of instance to wcss\n",
    "    # inertia_ attribute is the sum of squared distances of samples to their closest cluster center.\n",
    "\n",
    "# 3-1-d: assign title to the elbow method plot using plt.title()\n",
    "\n",
    "# 3-1-e: assign x axis label using plt.xlabel()\n",
    "\n",
    "# 3-1-f: assign y axis label using plt.ylabel()\n",
    "\n",
    "# 3-1-g: use sns.pointplot to draw a line plot of K vs. Within Cluster Sum of Squares (WCSS)\n",
    "\n",
    "# 3-1-h: show the plot using plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3-2 Silhouette Method to compare between K = 4,5,6\n",
    "\n",
    "# 3-2-a: import silhouette_score library \n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# 3-2-b: initiate a list called si\n",
    "\n",
    "# 3-2-c-1: because we are interested in reviewing the k values between 4,5,6 \n",
    "# build a for loop looping through k values between 4,7\n",
    "\n",
    "    # 3-2-c-2: initiate the instance of KMeans by setting the number of clusters as k to replicate the\n",
    "    # result it is crucial to select any number but a number and define the random_state parameter\n",
    "    \n",
    "    # 3-2-c-3: use the fit() function on the standardized data set into the KMeans instance\n",
    "    \n",
    "    # 3-2-c-4: assign labels_ attribute of the instance to the variable called lables\n",
    "    # labels_ attribute is labels or cluster of each point\n",
    "    \n",
    "    # 3-2-c-5: apply the silhouette_score function which returns the mean silhouette coefficient of the\n",
    "    # overall samples using the euclidean distance\n",
    "\n",
    "# 3-2-d: assign title to the silhouette method plot using plt.title()\n",
    "\n",
    "# 3-2-e: assign x axis label using plt.xlabel()\n",
    "\n",
    "# 3-2-f: assign y axis label using plt.ylabel()\n",
    "\n",
    "# 3-2-g: use sns.pointplot to draw a line plot of K vs. Silhouette score (si)\n",
    "\n",
    "# 3-2-h: show the plot using plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. test & learn\n",
    "\n",
    "# 4-1: initialize `KMeans` with 6 clusters\n",
    "\n",
    "# 4-2: fit the model on the pre-processed standardized dataset\n",
    "\n",
    "# 4-3: assign the generated labels to a new column\n",
    "\n",
    "# 4-4: group the dataset by the segment labels and calculate average feature values\n",
    "\n",
    "# 4-5: sort the cluster by average age\n",
    "\n",
    "# 4-6: add a new column 'sampleSize' to ensure the cluster size is meaningful\n",
    "\n",
    "# 4-7: print the average column values per each segment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Interpretation and discuss clusters with k = \n",
    "\n",
    "Cluster 1: \n",
    "Cluster 2: \n",
    "Cluster 3: \n",
    "Cluster 4: \n",
    "Cluster 5: \n",
    "Cluster 6: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
